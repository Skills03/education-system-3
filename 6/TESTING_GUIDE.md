# Testing Guide - Cognitive Teaching System

## ğŸš€ Quick Test

```bash
cd /home/mahadev/Desktop/dev/education/6
./run_test.sh
```

This will:
1. Start the server (if not running)
2. Run comprehensive automated tests
3. Show results summary

---

## ğŸ“‹ Manual Testing Steps

### 1. Start Server

```bash
python3 server.py
```

Expected output:
```
ğŸ“ SPECIALIZED TEACHING SYSTEM - INTELLIGENT AGENT ROUTING
...
ğŸ§  COGNITIVE FEATURES:
  â€¢ Concept-based limits    (max 3 concepts per response)
  â€¢ Sequential tool chains  (each tool builds on previous)
  â€¢ Pacing delays           (2s absorption time)
  â€¢ Persistent memory       (.claude/CLAUDE.md)
```

### 2. Test Agent Routing

Open browser: `http://localhost:5000`

#### Test Explainer Agent:
- Query: "Explain what variables are"
- Expected: Routes to **explainer** agent
- Look for: ğŸ“ routing message

#### Test Reviewer Agent:
- Query: "Review this code: def add(x,y): return x+y"
- Expected: Routes to **reviewer** agent
- Look for: ğŸ” routing message

#### Test Challenger Agent:
- Query: "Give me a challenge about loops"
- Expected: Routes to **challenger** agent
- Look for: ğŸ¯ routing message

#### Test Assessor Agent:
- Query: "Test my understanding of functions"
- Expected: Routes to **assessor** agent
- Look for: ğŸ“Š routing message

### 3. Test Memory Persistence

**Session 1:**
```
Query: "Teach me about variables"
```

Check `.claude/CLAUDE.md`:
```bash
cat .claude/CLAUDE.md
```

Should see:
- Session Count: increased
- Mastered/Learning Concepts: "variables"
- Session Log: entry for this session

**Session 2 (in same browser session):**
```
Query: "Now teach me about loops"
```

Agent should:
- âœ… Reference variables from Session 1
- âœ… Build loop examples using variables
- âœ… NOT re-explain variables

Check `.claude/CLAUDE.md` again:
- Session Count: increased again
- Both "variables" AND "loops" listed

### 4. Test Concept-Based Limits

Query: "Teach me Python basics"

Watch for:
1. **Concept Declaration**: Agent says "This response teaches N concepts: ..."
2. **Tool Count**: Should use 1-4 tools max (for 3 concepts)
3. **Sequential Order**: Tools build on each other

Example good sequence:
```
ğŸ”§ generate_concept_diagram (shows concept visually)
  â†“
ğŸ”§ show_code_example (demonstrates with code)
  â†“
ğŸ”§ create_interactive_challenge (practice)
```

### 5. Test Tool Sequencing

Query: "Explain arrays with examples"

Agent should:
- Use visual tool FIRST (diagram/visualization)
- Follow with code tool (references the diagram)
- Optionally add challenge (uses same code pattern)

âŒ BAD: Random unrelated tools
âœ… GOOD: Each tool builds on previous

---

## ğŸ§ª Automated Test Suite

Run comprehensive tests:
```bash
python3 test_comprehensive.py
```

Tests include:
1. âœ… Session Creation
2. âœ… Agent Routing (explainer, reviewer, challenger)
3. âœ… Memory Persistence (CLAUDE.md updates)
4. âœ… Concept Declaration
5. âœ… Tool Count Limits
6. âœ… Tool Sequencing

---

## ğŸ“Š Success Criteria

### Routing System: âœ…
- "Explain X" â†’ Explainer Agent
- "Review code" â†’ Reviewer Agent
- "Challenge me" â†’ Challenger Agent
- "Test me" â†’ Assessor Agent

### Memory System: âœ…
- `.claude/CLAUDE.md` exists
- Session count increments
- Concepts tracked (Mastered/Learning/Weak)
- Session history logged
- Agent reads memory before teaching
- Agent doesn't re-teach mastered concepts

### Concept System: âœ…
- Agent declares concepts upfront
- Maximum 3 concepts per response
- Tool usage scales with concept count
- Concept declaration format correct

### Tool System: âœ…
- Tools used sequentially (not randomly)
- Visual â†’ Code â†’ Practice pattern
- Each tool references previous
- 2-second pacing delays between outputs

---

## ğŸ› Troubleshooting

**Server won't start:**
```bash
# Kill existing processes
pkill -9 -f "python3 server.py"

# Check port
lsof -ti:5000 | xargs kill -9

# Restart
python3 server.py
```

**Tests fail:**
```bash
# Check server logs
tail -f /tmp/teaching_server.log

# Check CLAUDE.md
cat .claude/CLAUDE.md

# Reset CLAUDE.md
git checkout .claude/CLAUDE.md
```

**Import errors:**
```bash
pip3 install flask flask-cors anthropic sseclient-py
```

---

## ğŸ“ˆ Expected Test Results

**Passing Score: 80%+**

Example output:
```
ğŸ“Š TEST SUMMARY
================================================================================
ğŸ¯ Agent Routing: 3 tests
  âœ… Explainer routing
  âœ… Reviewer routing
  âœ… Challenger routing

ğŸ’¾ Memory Persistence: 2 tests
  âœ… Session count
  âœ… Concept recorded

ğŸ§  Concept System: 1 tests
  âœ… Declaration

ğŸ”§ Tool Management: 2 tests
  âœ… Count
  âœ… Sequencing

================================================================================
ğŸ“ˆ FINAL SCORE: 8/8 (100%)
ğŸ‰ SYSTEM FULLY OPERATIONAL
```

---

## ğŸ¯ What We're Testing

### 1. Intelligent Agent Routing
- Pattern-based query classification
- Confidence scoring
- Specialist agent selection

### 2. Persistent Memory
- `.claude/CLAUDE.md` read/write
- SDK `setting_sources=["project"]`
- Cross-session knowledge retention

### 3. Cognitive Load Management
- 3-concept maximum per response
- Working memory constraints
- Information density control

### 4. Tool Orchestration
- Sequential chaining validation
- Concept-based tool selection
- Pacing delays for absorption

---

## ğŸ“ Next Steps After Testing

If tests pass (80%+):
âœ… System is production-ready
âœ… Memory persistence working
âœ… Routing working
âœ… Concept limits enforced

If tests partially pass (60-79%):
âš ï¸ Check specific failing tests
âš ï¸ Review server logs
âš ï¸ Verify CLAUDE.md permissions

If tests fail (<60%):
âŒ Check server is actually running
âŒ Verify all dependencies installed
âŒ Review error messages
